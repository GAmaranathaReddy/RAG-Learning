{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Retrieval Augmented Generation","text":""},{"location":"#what-is-retrieval-augmented-generation-or-rag","title":"What Is Retrieval Augmented Generation, or RAG?","text":"<p>Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. This is done by retrieving data/documents relevant to a question or task and providing them as context for the LLM. RAG has shown success in support chatbots and Q&amp;A systems that need to maintain up-to-date information or access domain-specific knowledge. (or)</p> <p>Retrieval-Augmented Generation (RAG) is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts. (or)</p>"},{"location":"aws/aws/","title":"How can AWS support your Retrieval-Augmented Generation requirements?","text":"<p>Amazon Bedrock is a fully-managed service that offers a choice of high-performing foundation models\u2014along with a broad set of capabilities\u2014to build generative AI applications while simplifying development and maintaining privacy and security. With knowledge bases for Amazon Bedrock, you can connect FMs to your data sources for RAG in just a few clicks. Vector conversions, retrievals, and improved output generation are all handled automatically.</p> <p>For organizations managing their own RAG, Amazon Kendra is a highly-accurate enterprise search service powered by machine learning. It provides an optimized Kendra Retrieve API that you can use with Amazon Kendra\u2019s high-accuracy semantic ranker as an enterprise retriever for your RAG workflows. For example, with the Retrieve API, you can:</p> <p>Retrieve up to 100 semantically-relevant passages of up to 200 token words each, ordered by relevance. Use pre-built connectors to popular data technologies like Amazon Simple Storage Service, SharePoint, Confluence, and other websites. Support a wide range of document formats such as HTML, Word, PowerPoint, PDF, Excel, and text files. Filter responses based on those documents that the end-user permissions allow. Amazon also offers options for organizations who want to build more custom generative AI solutions. Amazon SageMaker JumpStart is a ML hub with FMs, built-in algorithms, and prebuilt ML solutions that you can deploy with just a few clicks. You can speed up RAG implementation by referring to existing SageMaker notebooks and code examples.</p> <p>Get started with Retrieval-Augmented Generation on AWS by creating a free account today</p>"},{"location":"azure/azure/","title":"Approaches for RAG with Azure AI Search","text":""},{"location":"azure/azure/#architecture-daigram","title":"Architecture daigram","text":""},{"location":"azure/azure/#microsoft-has-several-built-in-implementations-for-using-azure-ai-search-in-a-rag-solution","title":"Microsoft has several built-in implementations for using Azure AI Search in a RAG solution.","text":""},{"location":"azure/azure/#azure-ai-studio","title":"Azure AI Studio:","text":"<p>Use a vector index and retrieval augmentation. AI Studio</p>"},{"location":"azure/azure/#azure-openai-studio","title":"Azure OpenAI Studio:","text":"<p>Use a search index with or without vectors. Azure OpenAI</p>"},{"location":"azure/azure/#azure-machine-learning","title":"Azure Machine Learning,","text":"<p>Use a search index as a vector store in a prompt flow. Vector index in prompt flow</p> <p>Curated approaches make it simple to get started, but for more control over the architecture, you need a custom solution. These templates create end-to-end solutions in</p>"},{"location":"azure/azure/#references","title":"References:","text":"<p>Azure Reference link</p> <p>Azure GPT-RAG</p>"},{"location":"gcp/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":""},{"location":"open-source/wip/","title":"Build an LLM RAG Chatbot With LangChain","text":"<p>You\u2019ve likely interacted with large language models (LLMs), like the ones behind OpenAI\u2019s ChatGPT, and experienced their remarkable ability to answer questions, summarize documents, write code, and much more. While LLMs are remarkable by themselves, with a little programming knowledge, you can leverage libraries like LangChain to create your own LLM-powered chatbots that can do just about anything.</p> <p>In an enterprise setting, one of the most popular ways to create an LLM-powered chatbot is through retrieval-augmented generation (RAG). When you design a RAG system, you use a retrieval model to retrieve relevant information, usually from a database or corpus, and provide this retrieved information to an LLM to generate contextually relevant responses.</p> <p>In this tutorial, you\u2019ll step into the shoes of an AI engineer working for a large hospital system. You\u2019ll build a RAG chatbot in LangChain that uses Neo4j to retrieve data about the patients, patient experiences, hospital locations, visits, insurance payers, and physicians in your hospital system.</p> <p>In this tutorial, you\u2019ll learn how to:</p> <ul> <li>Use LangChain to build custom chatbots</li> <li>Design a chatbot using your understanding of the business requirements and hospital system data</li> <li>Work with graph databases</li> <li>Set up a Neo4j AuraDB instance</li> <li>Build a RAG chatbot that retrieves both structured and unstructured data from Neo4j</li> <li>Deploy your chatbot with FastAPI and Streamlit. <p>E2E Exmaple</p>"},{"location":"rag/benefits/","title":"Benefits of Retrieval augmented generation","text":"<p>RAG formulations can be applied to various NLP applications, including chatbots, question-answering systems, and content generation, where correct information retrieval and natural language generation are critical. The key advantages RAG provides include: The RAG approach has a number of key benefits, including:</p>"},{"location":"rag/benefits/#providing-up-to-date-and-accurate-responses","title":"Providing up-to-date and accurate responses","text":"<p>RAG ensures that the response of an LLM is not based solely on static, stale training data. Rather, the model uses up-to-date external data sources to provide responses.</p>"},{"location":"rag/benefits/#reducing-inaccurate-responses-or-hallucinations","title":"Reducing inaccurate responses, or hallucinations","text":"<p>By grounding the LLM model's output on relevant, external knowledge, RAG attempts to mitigate the risk of responding with incorrect or fabricated information (also known as hallucinations). Outputs can include citations of original sources, allowing human verification.</p>"},{"location":"rag/benefits/#providing-domain-specific-relevant-responses","title":"Providing domain-specific, relevant responses","text":"<p>Using RAG, the LLM will be able to provide contextually relevant responses tailored to an organization's proprietary or domain-specific data.</p>"},{"location":"rag/benefits/#being-efficient-and-cost-effective","title":"Being efficient and cost-effective","text":"<p>Compared to other approaches to customizing LLMs with domain-specific data, RAG is simple and cost-effective. Organizations can deploy RAG without needing to customize the model. This is especially beneficial when models need to be updated frequently with new data.</p>"},{"location":"rag/benefits/#cost-effective-implementation","title":"Cost-effective implementation","text":"<p>Chatbot development typically begins using a foundation model. Foundation models (FMs) are API-accessible LLMs trained on a broad spectrum of generalized and unlabeled data. The computational and financial costs of retraining FMs for organization or domain-specific information are high. RAG is a more cost-effective approach to introducing new data to the LLM. It makes generative artificial intelligence (generative AI) technology more broadly accessible and usable.</p>"},{"location":"rag/benefits/#current-information","title":"Current information","text":"<p>Even if the original training data sources for an LLM are suitable for your needs, it is challenging to maintain relevancy. RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.</p>"},{"location":"rag/benefits/#enhanced-user-trust","title":"Enhanced user trust","text":"<p>RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.</p>"},{"location":"rag/benefits/#more-developer-control","title":"More developer control","text":"<p>With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. In addition, they can also troubleshoot and make fixes if the LLM references incorrect information sources for specific questions. Organizations can implement generative AI technology more confidently for a broader range of applications.</p>"},{"location":"rag/benefits/#improved-relevance-and-accuracy","title":"Improved relevance and accuracy","text":"<p>By incorporating a retrieval component, RAG models can access external knowledge sources, ensuring the generated text is grounded in accurate and up-to-date information. This leads to more contextually relevant and accurate responses, reducing hallucinations in question answering and content generation.</p>"},{"location":"rag/benefits/#contextual-coherence","title":"Contextual coherence","text":"<p>Retrieval-based models provide context for the generation process, making generating coherent and contextually appropriate text easier. This leads to more cohesive and understandable responses, as the generation component can build upon the retrieved information.</p>"},{"location":"rag/benefits/#handling-open-domain-queries","title":"Handling open-domain queries","text":"<p>RAG models excel in taking open-domain questions where the required information may not be in the training data. The retrieval component can fetch relevant information from a vast knowledge base, allowing the model to provide answers or generate content on various topics.</p>"},{"location":"rag/benefits/#reduced-generation-bias","title":"Reduced generation bias","text":"<p>Incorporating retrieval can help mitigate some inherent biases in purely generative models. By relying on existing information from a diverse range of sources, RAG models can generate less biased and more objective responses.</p>"},{"location":"rag/benefits/#efficient-computation","title":"Efficient computation","text":"<p>Retrieval-based models can be computationally efficient for tasks where the knowledge base is already available and structured. Instead of generating responses from scratch, they can retrieve and adapt existing information, reducing the computational cost.</p>"},{"location":"rag/benefits/#multi-modal-capabilities","title":"Multi-modal capabilities","text":"<p>RAG models can be extended to work with multiple modalities, such as text and images. This allows them to generate contextually relevant text to textual and visual content, opening up possibilities for applications in image captioning, content summarization, and more.</p>"},{"location":"rag/benefits/#customization-and-fine-tuning","title":"Customization and fine-tuning","text":"<p>RAG models can be customized for specific domains or use cases. This adaptability makes them suitable for various applications, including domain-specific chatbots, customer support, and information retrieval systems.</p>"},{"location":"rag/benefits/#human-ai-collaboration","title":"Human-AI Collaboration","text":"<p>RAG models can assist humans in information retrieval tasks by quickly summarizing and presenting relevant information from a knowledge base, reducing the time and effort required for manual search.</p>"},{"location":"rag/important/","title":"Why is Retrieval-Augmented Generation important?","text":"<p>LLMs are a key artificial intelligence (AI) technology powering intelligent chatbots and other natural language processing (NLP) applications. The goal is to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.</p>"},{"location":"rag/important/#what-challenges-does-the-retrieval-augmented-generation-approach-solve","title":"What challenges does the retrieval augmented generation approach solve?","text":""},{"location":"rag/important/#problem-1-llm-models-do-not-know-your-data","title":"Problem 1: LLM models do not know your data","text":"<p>LLMs use deep learning models and train on massive datasets to understand, summarize and generate novel content. Most LLMs are trained on a wide range of public data so one model can respond to many types of tasks or questions. Once trained, many LLMs do not have the ability to access data beyond their training data cutoff point. This makes LLMs static and may cause them to respond incorrectly, give out-of-date answers or hallucinate when asked questions about data they have not been trained on.</p>"},{"location":"rag/important/#problem-2-ai-applications-must-leverage-custom-data-to-be-effective","title":"Problem 2: AI applications must leverage custom data to be effective","text":"<p>For LLMs to give relevant and specific responses, organizations need the model to understand their domain and provide answers from their data vs. giving broad and generalized responses. For example, organizations build customer support bots with LLMs, and those solutions must give company-specific answers to customer questions. Others are building internal Q&amp;A bots that should answer employees' questions on internal HR data. How do companies build such solutions without retraining those models?</p>"},{"location":"rag/important/#solution-retrieval-augmentation-is-now-an-industry-standard","title":"Solution: Retrieval augmentation is now an industry standard","text":"<p>An easy and popular way to use your own data is to provide it as part of the prompt with which you query the LLM model. This is called retrieval augmented generation (RAG), as you would retrieve the relevant data and use it as augmented context for the LLM. Instead of relying solely on knowledge derived from the training data, a RAG workflow pulls relevant information and connects static LLMs with real-time data retrieval.</p> <p>With RAG architecture, organizations can deploy any LLM model and augment it to return relevant results for their organization by giving it a small amount of their data without the costs and time of fine-tuning or pretraining the model.</p>"},{"location":"rag/important/#key-challenges-of-llms-include","title":"key challenges of LLMs include:","text":"<ul> <li>Presenting false information when it does not have the answer.</li> <li>Presenting out-of-date or generic information when the user expects a specific, current response.</li> <li>Creating a response from non-authoritative sources.</li> <li>Creating inaccurate responses due to terminology confusion, wherein different training sources use the same terminology to talk about different things.</li> </ul> <p>You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!</p> <p>RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources. Organizations have greater control over the generated text output, and users gain insights into how the LLM generates the response.</p>"},{"location":"rag/use-cases/","title":"What are the use cases for RAG?","text":"<p>There are many different use cases for RAG. The most common ones are:</p>"},{"location":"rag/use-cases/#customer-support-chatbots-question-and-answer-chatbots-conversational-agents","title":"Customer support chatbots/ Question and answer chatbots / Conversational agents","text":"<p>Incorporating LLMs with chatbots allows them to automatically derive more accurate answers from company documents and knowledge bases. Chatbots are used to automate customer support and website lead follow-up to answer questions and resolve issues quickly.</p> <p>In customer service, RAG can empower chatbots to give more accurate and contextually appropriate responses. By accessing up-to-date product information or customer data, these chatbots can provide better assistance, improving customer satisfaction. Ada, Amelia, and Rasa are real-world chatbots utilizing RAG, used by companies like Shopify, Bank of America, and Salesforce, to answer customer queries, resolve issues, complete tasks, and collect feedback.</p> <p>LLMs can be customised to product/service manuals, domain knowledge, guidelines, etc. using RAG. The agent can also route users to more specialised agents depending on their query. SearchUnify has an LLM+RAG powered conversational agent for their users.</p>"},{"location":"rag/use-cases/#search-augmentation","title":"Search augmentation","text":"<p>Incorporating LLMs with search engines that augment search results with LLM-generated answers can better answer informational queries and make it easier for users to find the information they need to do their jobs.</p>"},{"location":"rag/use-cases/#knowledge-engine","title":"Knowledge engine","text":"<p>Ask questions on your data (e.g., HR, compliance documents): Company data can be used as context for LLMs and allow employees to get answers to their questions easily, including HR questions related to benefits and policies and security and compliance questions.</p>"},{"location":"rag/use-cases/#business-intelligence-and-analysis","title":"Business intelligence and analysis","text":"<p>Businesses can use RAG to generate market analysis reports or insights. By retrieving and incorporating the latest market data and trends, RAG can offer more accurate and actionable business intelligence, as utilized by platforms like IBM Watson Assistant, Google Cloud Dialogflow, Microsoft Azure Bot Service, and Rasa.</p>"},{"location":"rag/use-cases/#healthcare-information-systems","title":"Healthcare information systems","text":"<p>In healthcare, RAG can improve systems that provide medical information or advice. By accessing the latest medical research and guidelines, such systems can offer more accurate and safe medical recommendations. HealthTap and BuoyHealth are healthcare chatbots using RAG to provide patients with health condition information, medication advice, doctor and hospital finding services, appointment scheduling, and prescription refills.</p>"},{"location":"rag/use-cases/#legal-research","title":"Legal research","text":"<p>Legal professionals can use RAG to quickly pull relevant case laws, statutes, or legal writings, streamlining the research process and ensuring more comprehensive legal analysis. Lex Machina and Casetext are real-world legal research chatbots using RAG to assist lawyers in finding case law, statutes, and regulations from various sources like Westlaw, LexisNexis, and Bloomberg Law, providing summaries, answering legal queries, and identifying potential legal issues.</p>"},{"location":"rag/use-cases/#content-creation","title":"Content creation","text":"<p>In content creation, like writing articles or reports, RAG can improve the quality and relevance of the output. It does this by pulling in accurate, current information from various sources, thereby enriching the content with factual details. Jasper and ShortlyAI are examples of real-world tools that use RAG for creating content.</p>"},{"location":"rag/use-cases/#educational-tools","title":"Educational tools","text":"<p>RAG can be used in educational platforms to provide students with detailed explanations and contextually relevant examples, drawing from a vast range of educational materials. Notably, Duolingo uses RAG for personalized language instruction and feedback, while Quizlet employs it to generate tailored practice questions and provide user-specific feedback.</p>"},{"location":"rag/use-cases/#document-question-answering-systems","title":"Document Question Answering Systems","text":"<p>By providing access to proprietary enterprise document to an LLM, the responses are limited to what is provided within them. A retriever can search for the most relevant documents and provide the information to the LLM. Check out this blog for an example \u2014 https://medium.com/mlearning-ai/conversing-with-documents-unleashing-the-power-of-llms-and-langchain-397838127fd</p>"},{"location":"rag/use-cases/#real-time-event-commentary","title":"Real-time Event Commentary","text":"<p>Imagine an event like a sports or a new event. A retriever can connect to real-time updates/data via APIs and pass this information to the LLM to create a virtual commentator. These can further be augmented with Text To Speech models.IBM leveraged the technology for commentary during the 2023 US Open</p>"},{"location":"rag/use-cases/#content-generation","title":"Content Generation","text":"<p>The widest use of LLMs has probably been in content generation. Using RAG, the generation can be personalised to readers, incorporate real-time trends and be contextually appropriate. Yarnit is an AI based content marketing platform that uses RAG for multiple tasks.</p>"},{"location":"rag/use-cases/#personalised-recommendation","title":"Personalised Recommendation","text":"<p>Recommendation engines have been a game changes in the digital economy. LLMs are capable of powering the next evolution in content recommendations. Check out Aman Chadha's blog on the utility of LLMs in recommendation systems.</p>"},{"location":"rag/workflow/","title":"How does Retrieval-Augmented Generation work?","text":"<p>Without RAG, the LLM takes the user input and creates a response based on information it was trained on\u2014or what it already knows. With RAG, an information retrieval component is introduced that utilizes the user input to first pull information from a new data source. The user query and the relevant information are both given to the LLM. The LLM uses the new knowledge and its training data to create better responses. The following sections provide an overview of the process.</p>"},{"location":"rag/workflow/#create-external-data","title":"Create external data","text":"<p>The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.</p>"},{"location":"rag/workflow/#retrieve-relevant-information","title":"Retrieve relevant information","text":"<p>The next step is to perform a relevancy search. The user query is converted to a vector representation and matched with the vector databases. For example, consider a smart chatbot that can answer human resource questions for an organization. If an employee searches, \"How much annual leave do I have?\" the system will retrieve annual leave policy documents alongside the individual employee's past leave record. These specific documents will be returned because they are highly-relevant to what the employee has input. The relevancy was calculated and established using mathematical vector calculations and representations.</p>"},{"location":"rag/workflow/#augment-the-llm-prompt","title":"Augment the LLM prompt","text":"<p>Next, the RAG model augments the user input (or prompts) by adding the relevant retrieved data in context. This step uses prompt engineering techniques to communicate effectively with the LLM. The augmented prompt allows the large language models to generate an accurate answer to user queries.</p>"},{"location":"rag/workflow/#update-external-data","title":"Update external data","text":"<p>The next question may be\u2014what if the external data becomes stale? To maintain current information for retrieval, asynchronously update the documents and update embedding representation of the documents. You can do this through automated real-time processes or periodic batch processing. This is a common challenge in data analytics\u2014different data-science approaches to change management can be used.</p> <p></p>"},{"location":"sap/arch/","title":"SAP Hana Cloud - Vector DB - RAG","text":""},{"location":"sap/introduction/","title":"SAP Hana Cloud - Vector DB - RAG","text":"<p>The SAP HANA Cloud's Vector Engine addresses the challenges associated with solving GenAI scenarios. The Vector Engine is a new, multi-model engine that empowers users to store and query vector embeddings seamlessly, treating them like any other data types.</p> <p>Here, we document the steps involved to in using the HANA's Vector Search Engine capability together with the IES GenAI Platform LLMs.</p> <p>We have a set of PoCs that we have conducted for using our IES GenAI Platform LLMs for generating embedding and also how one could build a simple Retrieval Augmented Generation use case using these LLMs along with HANA DB.</p> <p>Link to the VectorDB PoC Notebooks can be found here:</p> <ul> <li>Generating and Storing embeddings inside HanaDB using AICore RestAPI</li> <li>Generating and Storing embeddings inside HanaDB using AICore PythonSDK</li> <li>Retrieval Augmented Generation (RAG) use case with HanaDB using AICore RestAPI</li> <li>Retrieval Augmented Generation (RAG) use case with HanaDB using AICore PythonSDK</li> </ul>"},{"location":"sap/introduction/#prerequisites","title":"Prerequisites","text":"<ul> <li>SAP AICore</li> <li>SAP HANA Cloud</li> <li>SAP GenAI HUB</li> <li>SAP LLM Commons (or)</li> <li>SAP AICore LLM</li> </ul>"},{"location":"sap/new_hana/","title":"What's new in SAP HANA Vector Search feature","text":"<p>The Vector Engine in SAP HANA Cloud will:</p> <ul> <li>Facilitate storage of vector embeddings by introducing:</li> <li>A new data type named REAL_VECTOR</li> <li>A vector constructor TO_REAL_VECTOR, to create vector from strings</li> <li> <p>Facilitate similarity search queries and analysis by introducing:</p> </li> <li> <p>Two new distance calculating similarity search functions, L2Distance() and cosine_similarity(), to enhance the platform's capability to compute vector similarity.</p> </li> </ul> <p>These features in turn will facilitate:</p> <ul> <li>Storage and querying of vector embeddings in SAP HANA Cloud through SQL</li> <li>In-memory similarity searches to support retrieval-augmented generation (RAG) patterns</li> <li>Power to combine business data with graph, spatial, document, and vector data all on a single platform</li> <li>Storage and retrieval of contextual information for GenAI and intelligent data applications as vector embeddings</li> </ul> <p>In the next section we provide steps involved in using these new features.</p>"},{"location":"sap/rag/","title":"Retrieval Augmented Generation (RAG) Usecase","text":""},{"location":"sap/rag/#overview","title":"Overview","text":"<p>A RAG system comprises of a Semantic Search and Retrieval functionality, Leverage vector embeddings to enhance semantic search capabilities, enabling users to find relevant information quickly.</p> <p>The image below gives you an idea of how the RAG use case can be built using the SAP HANA Cloud Vector Engine along with the GenAI Platform LLMs: </p> <p>The following notebooks provides you with the step by step guidance on building a Retrieval Augmented Generation use case using HanaDBs Vector Search and AICores RestAPIs and PythonSDKs:</p> <ul> <li>Retrieval Augmented Generation (RAG) use case with HanaDB using AICore RestAPI</li> <li>Retrieval Augmented Generation (RAG) use case with HanaDB using AICore PythonSDK</li> </ul>"},{"location":"sap/rag/#similarity-search-with-hanadb","title":"Similarity Search with HanaDB:","text":"<p>Here we have defined a run_vector_search function which takes in a query and then provide relevant context using a similarity search function. HanaDB provides two new distance calculating similarity search function: L2Distance() and cosine_similarity(), to compute vector similarity.</p> <p>Similarity search</p> <pre><code># Wrapping HANA vector search in a function: Here we are using the Cosine Similarity as the Similarity Search function.\ndef run_vector_search(query: str, embedding_model, metric=\"COSINE_SIMILARITY\", k=4):\n    if metric == 'L2DISTANCE':\n        sort = 'ASC'\n    else:\n        sort = 'DESC'\n    query_vector = embedding_model.embed_query(query)\n    sql = '''SELECT TOP {k} \"ID\", \"HEADER1\", \"HEADER2\", \"TEXT\"\n        FROM \"GRAPH_DOCU_QRC3\"\n        ORDER BY \"{metric}\"(\"VECTOR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n    hdf = cc.sql(sql)\n    df_context = hdf.head(k).collect()\n    # context = ' '.join(df_context['TEXT'].astype('string'))\n    return df_context\n</code></pre>"},{"location":"sap/save_hana/","title":"Generating and storing embeddings inside the HANA DB","text":"<p>The following notebooks provides you with the step by step guidance on generating embeddings using AICores RestAPIs and PythonSDKs and storing them inside the Hana DB:</p> <ul> <li>Generating and Storing embeddings inside HanaDB using AICore RestAPI</li> <li>Generating and Storing embeddings inside HanaDB using AICore PythonSDK</li> </ul>"},{"location":"sap/save_hana/#hanadbs-new-data-type-real_vector","title":"HanaDBs new data type 'REAL_VECTOR'","text":"<p>A new data type named REAL_VECTOR has been introduced to encode and store the embeddings into. In the example below, we add a new column named \"VECTOR\" with the newly introduced datatype \"REAL_VECTOR\"</p> <p>Adding a REAL_VECTOR Column to your Database</p> <pre><code># Add REAL_VECTOR column\ncursor = cc.connection.cursor()\nsql_command = '''ALTER TABLE GRAPH_DOCU_QRC3 ADD (VECTOR REAL_VECTOR);'''\ncursor.execute(sql_command)cursor.close()\n</code></pre>"},{"location":"sap/save_hana/#new-constructor-to_real_vector-to-convert-embeddings-into-vectors","title":"New constructor \"TO_REAL_VECTOR\" to convert embeddings into vectors","text":"<p>We then can use the vector constructor TO_REAL_VECTOR, to encode vector from your generated embedding strings. Here we have the vectors inside the \"VECTOR_STR\" column which we feed to the TO_REAL_VECTOR constructor. This is then saved inside the \"VECTOR\" column</p> <p>Create Vectors from Embedding strings</p> <pre><code># Create vectors from embedding strings\ncursor = cc.connection.cursor()\nsql_command = '''UPDATE GRAPH_DOCU_QRC3 SET VECTOR = TO_REAL_VECTOR(VECTOR_STR);'''\ncursor.execute(sql_command)\ncursor.close()\n</code></pre> <p>View the new datatype inside your HANA Database:</p> <p></p>"},{"location":"sap/save_hana/#using-the-to_nvarchar-function-to-convert-vectors-to-embeddings","title":"Using the \"TO_NVARCHAR\" function to convert vectors to embeddings","text":"<p>Decoding the REAL_VECTOR datatype back to vector strings. We can do this using the TO_NVARCHAR constructor. This then decodes the generation REAL_VECTOR type into embeddings:</p> <p>Decoding embeddings from vectors</p> <pre><code># The TO_NVARCHAR function is used here to dercode REAL_VECTORS into vector embeddings\n\nhdf = cc.sql('''SELECT TOP 10 \"Key\", \"Abstract\", TO_NVARCHAR(VECTOR_RE) AS VECTOR_STR FROM NEWS_APPL2 WHERE VECTOR_RE IS NOT NULL''')\ndf_abstract = hdf.collect()\ndf_abstract\n</code></pre>"},{"location":"sap/wip/","title":"\ud83d\udea7 We're Building Something Amazing! \ud83d\udea7","text":""},{"location":"tags/","title":"Tags","text":"<p>This page shows a list of pages indexed by their tags:</p>"}]}